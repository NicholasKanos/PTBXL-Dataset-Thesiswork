{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c1606e-fbb6-44af-98f8-4f4c32ad0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Notebook\n",
    "# Imports\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74364724-006d-459b-828a-e0b01914bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PTB-XL Dataset\n",
    "PTBXL_URL = \"https://physionet.org/static/published-projects/ptb-xl/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1.zip\"\n",
    "\n",
    "def download_ptbxl(data_dir=\"data/raw\", unzip=True, overwrite=False):\n",
    "    data_path = Path(data_dir)\n",
    "    zip_path = data_path / \"ptbxl.zip\"\n",
    "    extract_path = data_path / \"ptbxl\"\n",
    "\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if extract_path.exists() and not overwrite:\n",
    "        print(f\"Dataset already exists at {extract_path}. Skipping download.\")\n",
    "        return extract_path\n",
    "\n",
    "    if not zip_path.exists() or overwrite:\n",
    "        print(f\"Downloading PTB-XL dataset to {zip_path}...\")\n",
    "        response = requests.get(PTBXL_URL, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024\n",
    "\n",
    "        with open(zip_path, 'wb') as f, tqdm(total=total_size, unit='iB', unit_scale=True, desc=\"Downloading\") as bar:\n",
    "            for chunk in response.iter_content(chunk_size=block_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    bar.update(len(chunk))\n",
    "        print(\"Download complete.\")\n",
    "\n",
    "    if unzip:\n",
    "        try:\n",
    "            print(f\"Extracting PTB-XL dataset to {extract_path}...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_path)\n",
    "            print(\"Extraction complete.\")\n",
    "        except zipfile.BadZipFile:\n",
    "            print(\"Corrupted zip file detected. Deleting and retrying...\")\n",
    "            zip_path.unlink()\n",
    "            return download_ptbxl(data_dir, unzip, overwrite=True)\n",
    "\n",
    "    return extract_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5c514d-96d5-41dd-a917-db3d968e0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto Shape fix\n",
    "\n",
    "class PTBXL_Dataset(Dataset):\n",
    "    def __init__(self, df, labels, base_dir):\n",
    "        self.df = df\n",
    "        self.labels = labels\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        full_path = os.path.join(self.base_dir, row['filename_lr'])\n",
    "        signal = load_ecg(full_path)\n",
    "        # This ensures all output is [12, 5000] (channels, time)\n",
    "        if signal.shape[1] == 12:\n",
    "            signal = signal.T\n",
    "        return torch.tensor(signal, dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f7b77f-7c7a-45bc-950d-3ab424ddda1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "dataset_path = Path(\"data/raw/ptbxl/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1\")\n",
    "ptbxl_path = dataset_path / \"ptbxl_database.csv\"\n",
    "waveform_path = dataset_path\n",
    "\n",
    "# Load dataframe\n",
    "df = pd.read_csv(ptbxl_path)\n",
    "df['scp_codes'] = df['scp_codes'].apply(ast.literal_eval)\n",
    "df['scp_keys'] = df['scp_codes'].apply(lambda x: list(x.keys()))\n",
    "target_labels = ['NORM', 'AFIB', 'PVC', 'LVH', 'IMI', 'ASMI', 'LAFB', 'IRBBB']\n",
    "df['scp_filtered'] = df['scp_keys'].apply(lambda x: [k for k in x if k in target_labels])\n",
    "df = df[df['scp_filtered'].map(len) > 0]\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=target_labels)\n",
    "y = mlb.fit_transform(df['scp_filtered'])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Helper: load ECG\n",
    "def load_ecg(record_path):\n",
    "    record = wfdb.rdrecord(record_path)\n",
    "    return record.p_signal  # shape: [5000, 12]\n",
    "\n",
    "# PyTorch Dataset\n",
    "class PTBXL_Dataset(Dataset):\n",
    "    def __init__(self, df, labels, base_dir):\n",
    "        self.df = df\n",
    "        self.labels = labels\n",
    "        self.base_dir = base_dir\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        full_path = os.path.join(self.base_dir, row['filename_lr'])\n",
    "        signal = load_ecg(full_path).T  # [12, 5000]\n",
    "        return torch.tensor(signal, dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "# KFold for Transformer Models\n",
    "\n",
    "kf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "afib_idx = target_labels.index('AFIB')\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df, y)):\n",
    "   \n",
    "    # Split data\n",
    "    X_train, X_val = df.iloc[train_idx], df.iloc[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # OVERSAMPLE AFIB in training set \n",
    "    afib_mask = y_train[:, afib_idx] == 1\n",
    "    X_train_afib = X_train[afib_mask]\n",
    "    y_train_afib = y_train[afib_mask]\n",
    "\n",
    "    # Oversample AFIB rows (here: double, adjust multiplier as needed)\n",
    "    X_train_oversampled = pd.concat([X_train, X_train_afib])\n",
    "    y_train_oversampled = np.concatenate([y_train, y_train_afib])\n",
    "\n",
    "    # Shuffle after oversampling\n",
    "    shuffler = np.random.permutation(len(X_train_oversampled))\n",
    "    X_train_oversampled = X_train_oversampled.iloc[shuffler]\n",
    "    y_train_oversampled = y_train_oversampled[shuffler]\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = PTBXL_Dataset(X_train, y_train, waveform_path)\n",
    "test_dataset = PTBXL_Dataset(X_test, y_test, waveform_path)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d55c8001-f1f9-49b7-8bae-78f9ce032c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model Definition\n",
    "class ECG_Transformer(nn.Module):\n",
    "    def __init__(self, seq_len=5000, num_features=12, d_model=32, nhead=2, num_layers=2, num_classes=8):\n",
    "        super(ECG_Transformer, self).__init__()\n",
    "        self.input_linear = nn.Linear(num_features, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.input_linear(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.global_avg_pool(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        pass\n",
    "        \n",
    "# Transformer CCN Model Definition\n",
    "class ECG_Transformer_CNN(nn.Module):\n",
    "    def __init__(self, seq_len=5000, num_features=12, d_model=32, nhead=2, num_layers=2, num_classes=8):\n",
    "        super(ECG_Transformer_CNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=num_features, out_channels=32, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=32, out_channels=d_model, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.global_avg_pool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "        pass\n",
    "\n",
    "# xResNet1D Model Definition\n",
    "class ConvLayer(nn.Sequential):\n",
    "    def __init__(self, ni, nf, ks=3, stride=1, padding=None, bias=None, act_cls=nn.ReLU):\n",
    "        if padding is None: padding = (ks - 1) // 2\n",
    "        conv = nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=padding, bias=bias or False)\n",
    "        bn = nn.BatchNorm1d(nf)\n",
    "        layers = [conv, bn]\n",
    "        if act_cls is not None:\n",
    "            layers.append(act_cls())\n",
    "        super().__init__(*layers)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, stride=1):\n",
    "        super().__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "            ConvLayer(ni, nf, stride=stride),\n",
    "            ConvLayer(nf, nf, act_cls=None)\n",
    "        )\n",
    "        self.idconv = nn.Identity() if ni == nf and stride == 1 else ConvLayer(ni, nf, ks=1, stride=stride, act_cls=None)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.convs(x) + self.idconv(x))\n",
    "\n",
    "class XResNet1d(nn.Sequential):\n",
    "    def __init__(self, layers, input_channels=12, num_classes=10):\n",
    "        self.block_sizes = [64, 128, 256, 512]\n",
    "        stem = [ConvLayer(input_channels, 64, ks=7, stride=2), nn.MaxPool1d(kernel_size=3, stride=2, padding=1)]\n",
    "        blocks = []\n",
    "        ni = 64\n",
    "        for i, n_blocks in enumerate(layers):\n",
    "            nf = self.block_sizes[i]\n",
    "            for j in range(n_blocks):\n",
    "                stride = 2 if j == 0 and i != 0 else 1\n",
    "                blocks.append(ResBlock(ni, nf, stride=stride))\n",
    "                ni = nf\n",
    "        head = [nn.AdaptiveAvgPool1d(1), nn.Flatten(), nn.Linear(ni, num_classes)]\n",
    "        super().__init__(*stem, *blocks, *head)\n",
    "        pass\n",
    "\n",
    "# xLSTM Model Definition\n",
    "class xLSTMECG(nn.Module):\n",
    "    def __init__(self, input_channels=12, num_classes=10, hidden_size=128, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=7, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = x.permute(0, 2, 1)  # [B, T, C]\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # last time step\n",
    "        return self.fc(x)\n",
    "        pass\n",
    "\n",
    "# ResNet1D Model Definition\n",
    "\n",
    "class BasicBlock1d(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        return self.relu(out + identity)\n",
    "\n",
    "class ResNet1d(nn.Module):\n",
    "    def __init__(self, block, layers, in_channels=12, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion)\n",
    "            )\n",
    "        layers = [block(self.inplanes, planes, stride, downsample=downsample)]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x).squeeze(-1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd3c9b07-73ff-4d03-90d5-b122765062f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selector \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Uncomment ONE model you want to use ===\n",
    "\n",
    "\n",
    "# model = ResNet1d(BasicBlock1d, [2, 2, 2, 2], in_channels=12, num_classes=len(target_labels))\n",
    "\n",
    "model = XResNet1d([2, 2, 2, 2], input_channels=12, num_classes=len(target_labels)).to(device)\n",
    "\n",
    "# model = xLSTMECG(input_channels=12, num_classes=len(target_labels)).to(device)\n",
    "\n",
    "# model = ECG_Transformer(num_classes=len(target_labels)).to(device)\n",
    "\n",
    "# model = ECG_Transformer_CNN(num_classes=len(target_labels)).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88a9f1bc-1cc0-4b42-8b36-9339806e72e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1791/1791 [00:42<00:00, 42.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1088, Accuracy: 0.9551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1791/1791 [00:40<00:00, 44.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.1042, Accuracy: 0.9574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1791/1791 [00:42<00:00, 41.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.0983, Accuracy: 0.9597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1791/1791 [00:40<00:00, 43.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.0963, Accuracy: 0.9612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 1791/1791 [00:40<00:00, 44.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.0896, Accuracy: 0.9640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 1791/1791 [00:40<00:00, 44.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.0845, Accuracy: 0.9663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 1791/1791 [00:43<00:00, 41.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.0793, Accuracy: 0.9678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 1791/1791 [00:43<00:00, 41.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.0748, Accuracy: 0.9698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 1791/1791 [00:44<00:00, 40.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.0678, Accuracy: 0.9731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 1791/1791 [00:43<00:00, 40.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.0622, Accuracy: 0.9756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "afib_index = target_labels.index('AFIB')  # Add this if you want to use a custom AFIB threshold\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for signals, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        signals, labels = signals.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(signals)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Compute predictions with thresholding\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = torch.zeros_like(probs)\n",
    "        # Custom threshold for AFIB\n",
    "        preds[:, afib_index] = (probs[:, afib_index] > 0.3).float()\n",
    "        # Default threshold for all other classes\n",
    "        for i in range(probs.shape[1]):\n",
    "            if i != afib_index:\n",
    "                preds[:, i] = (probs[:, i] > 0.5).float()\n",
    "\n",
    "        total_correct += (preds == labels).float().sum().item()\n",
    "        total_samples += preds.numel()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c789f4-b68e-4baf-babb-d32a0b35d3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for signals, labels in test_loader:\n",
    "        signals, labels = signals.to(device), labels.to(device)\n",
    "        outputs = model(signals)\n",
    "        y_true.append(labels.cpu().numpy())\n",
    "        y_pred.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "y_true = np.vstack(y_true)\n",
    "y_pred = np.vstack(y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true > 0.5, y_pred > 0.5, target_names=mlb.classes_))\n",
    "print(\"Macro ROC-AUC:\", roc_auc_score(y_true, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a85997-42ea-4543-a9a4-05bdea30e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    fpr, tpr, _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{label} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multi-Label ROC Curves')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0db200-a5d9-4275-b085-02df14b0158d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OSC PyTorch 2.7.1",
   "language": "python",
   "name": "pytorch_2.7.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
